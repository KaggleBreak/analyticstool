{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\vect}[1]{\\boldsymbol{#1}}$\n",
    "# 4\n",
    "# World Wide Web, Wikipedia, and Social Networks\n",
    "\n",
    "## Intoroduction\n",
    "- The WWW bases its success on the potential offered by the hypertext markup language (html).\n",
    "- It is possible to link documents and media with each other creating a network of information.\n",
    "- WWW developed a series of addresses of the form “www.oup.com\"\n",
    "- A hierarchical classifìcation, an almost infìnite series of documents can be mapped below this address.\n",
    "- The mapping is stored in specifìc servers named “domain name servers (DNS)\"\n",
    "- Any new page had to be found and put manually into this artificial taxonomy to be present in the list.\n",
    "- Became more and more difficult as the numbers exploded (where and how to find all new pages?) and the content became more and more complex (how to assess the category of a web page?) to classify.\n",
    "\n",
    "\n",
    "## Data from various sources\n",
    "### WWW\n",
    "- The WWW is a classic example of big data.\n",
    "- The largest coherent structure created by humans.\n",
    "- It is therefore of the utmost importance to be able to handle these series of data, and whenever possible to consider properly defined subsets of them.\n",
    "- Starting an exploration of the web, database from the University of Milan,Italy.\n",
    "    - At http://law.di.unimi.it we can find information on this site;\n",
    "    - http://law.di.unimi.it/datasets.php contains a series of data collected and storedm in compressed form;\n",
    "    - http://webgraph.di.unimi.it/ contains information about the Webgraph compressed graph format and instructions on how to extract it.\n",
    "    - [Download \".eu\" (http://law.di.unimi.it/webdata/eu-2005/)](http://law.di.unimi.it/webdata/eu-2005/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Code for loading the \".eu\" portion of the WWW in 2005\n",
    "# Modified by etc.\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "eu_DG = nx.DiGraph()\n",
    "eu_DG = nx.read_edgelist('./data/eu-2005_1M.arcs', create_using=nx.DiGraph())\n",
    "\n",
    "# generate the dictionary of node_is -> urls\n",
    "count = 0\n",
    "dic_nodid_urls={}\n",
    "\n",
    "with open(\"./data/eu-2005.urls\") as f:\n",
    "    for line in f:\n",
    "        dic_nodid_urls[str(count)] = line[:-1]\n",
    "        count += 1\n",
    "\n",
    "# generate the strongly connected component\n",
    "scc = [(len(c), c) for c in sorted(nx.strongly_connected_components(eu_DG),\n",
    "                                   key=len, reverse=True)][0][1]\n",
    "eu_DG_SCC = eu_DG.subgraph(scc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter\n",
    "- Twitter and Facebook are two clear cases where networks help in measuring social relationships.\n",
    "- \"tweets\"\n",
    "- \"retweets\"\n",
    "- \"following\"\n",
    "    - Is not reciprocal (i.e. if A follows B, not necessarily does B follow A).\n",
    "- Twitter and Facebook are two clear cases where networks help in measuring social relationships.\n",
    "- Twitter APIs : https://dev.twitter.com/docs\n",
    "- Python module : https://twython.readthedocs.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code for the opening of tweets with the API\n",
    "from twython import Twython\n",
    "\n",
    "APP_KEY='xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'\n",
    "APP_SECRET='xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'\n",
    "OAUTH_TOKEN='xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'\n",
    "OAUTH_TOKEN_SECRET='xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'\n",
    "\n",
    "twitter_connection=Twython(APP_KEY, APP_SECRET,\n",
    "                           OAUTH_TOKEN, OAUTH_TOKEN_SECRET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# How to get the timeline\n",
    "\n",
    "res=twitter_connection.get_home_timeline()\n",
    "for t in res[:5]:\n",
    "    print('Tweet:', t['text'], \n",
    "          '-', t['user']['name'], '(@' + t['user']['screen_name'] + ')')\n",
    "    print('Mentions:', end='')\n",
    "    for m in t['entities']['user_mentions']:\n",
    "        print(m['screen_name'], end='')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# How to get user information\n",
    "res = twitter_connection.show_user(screen_name='@BarackObama')\n",
    "print(res)\n",
    "print('Location: ', res['location'])\n",
    "print('Number of followers: ', res['followers_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Retreving tweets with eh \"search' function\n",
    "res = twitter_connection.search(q='#ebola', count=2)\n",
    "for t in res['statuses']:\n",
    "    print(\"Tweet:\", t['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikipedia\n",
    "- The various pages are interconnected forming one of <u>the largest thematic subnetworks</u> of the WWW.\n",
    "- Interest in this subset of WWW pages is based on a series of reasons.\n",
    "    - Wikipedia is a <u>well defined subgraph</u> of the WWW; indeed it forms a <u>thematic subset</u>,thereby creating a <u>natural laboratory</u> for WWW studies.\n",
    "    - Over time Wikipedia has developed in different languages, so that various subsets of Wikipedia of different sizes are now avaìlable. Furthermore,Wikipedia networks <u>allow us to test whether different cultures tend to organise web pages differently</u>.\n",
    "    - <u>All information on the Wikipedia graph is available</u>, even its <u>growth history</u>, with a <u>time stamp</u> for any additions to the system.\n",
    "    - Wikipedia pages tend (where possible) to cite other Wikipedia pages, so that the whole system is contained.\n",
    "- If the links connecting two pages (lemmas of the encyclopaedia) determine communities of concepts and ultimately define a bottom-up taxonomy of reciprocal concepts (as one would expect).\n",
    "- Download - https://dumps.wikimedia.org\n",
    "- Use a small portion of Wikipedia,that “[in Limba Sarda](https://sc.wikipedia.org/)\" (Sardinian),which is at the moment formed from about 4500 articles.\n",
    "- The structure of the Pagelinks and Page table\n",
    "    - https://www.mediawiki.org/wiki/Manual:Pagelinks_table\n",
    "    - https://www.mediawiki.org/wiki/Manual:Page_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Opening the Wikipeia Sardinian dump\n",
    "\n",
    "import _mysql\n",
    "\n",
    "scwiki_db = mysql.connect(host=\"locahost\", user=\"XXXXX\",\n",
    "                          passwd=\"XXXXX\", db=\"scwiki_db)\n",
    "\n",
    "scwiki_db.query(\"\"\"SELECT pagelinks.pl_from, page.page_id \n",
    "FROM page, pagelinks\n",
    "WHERE page.page_title = pagelinks.pltitle\"\"\")\n",
    "r=scwiki_db.use_result()\n",
    "f=open(\"./data/scwiki_edgelist.data\", 'w')\n",
    "res=r.fetch_row()\n",
    "while res!=():\n",
    "    f.wrtie(res[0][0]+\" \"+res[0][1]+\"\\n\")\n",
    "    res=r.fetch.row()\n",
    "f.close()\n",
    "\n",
    "scwiki_db.query(\"SELECT page.page_id, page.page_title FROM page\")\n",
    "r=scwiki_db.use_result()\n",
    "f=open(\"./data/scwiki_page_titles.dat\", 'w')\n",
    "res=r.fetch_row()\n",
    "while res != ():\n",
    "    f.write(res[0][0]+\" \"+res[0][1]+\"\\n\")\n",
    "    res = r.fetchrow()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using SQLite3 (appended by etc.)\n",
    "- SQL dumps were converted by following scriptor.\n",
    "    https://github.com/dumblob/mysql2sqlite\n",
    "- SQL import (in Un\\*x)\n",
    "\n",
    "    <code>#./mysql2sqlite scwiki-20170620-page.sql | sqlite3 scwiki_db<br>#./mysql2sqlite scwiki-20170620-pagelinks.sql.sql | sqlite3 scwiki_db</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Opening the Wikipeia Sardinian dump using SQLite3\n",
    "# Modified by etc.\n",
    "import sqlite3\n",
    "\n",
    "scwiki_db = sqlite3.connect(\"data/scwiki_db\")\n",
    "\n",
    "# extract the hyperlinks information\n",
    "sql = \"\"\"\n",
    "SELECT pagelinks.pl_from, page.page_id \\\n",
    "FROM page, pagelinks \\\n",
    "WHERE page.page_title = pagelinks.pl_title\n",
    "\"\"\"\n",
    "    \n",
    "with open(\"./data/scwiki_edgelist.dat\", 'w') as f:\n",
    "    for r in scwiki_db.execute(sql):\n",
    "#         print(str(r[0])+\" \"+str(r[1])+\"\\n\")\n",
    "        f.write(str(r[0]) + \" \" + str(r[1]) + \"\\n\")    \n",
    "\n",
    "# extract the title information\n",
    "sql = \"\"\"\n",
    "SELECT page.page_id, page.page_title FROM page\n",
    "\"\"\"\n",
    "with open(\"./data/scwiki_page_titles.dat\", 'w') as f:\n",
    "    for r in scwiki_db.execute(sql):\n",
    "#         print(str(r[0])+\" \"+str(r[1])+\"\\n\")\n",
    "        f.write(str(r[0]) + \" \" + str(r[1]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikipedia taxonomy\n",
    "- Since Wikipedia is a means of organising knowledge (Gonzaga et al. , 2001) , it is interesting to check whether the structures arising from different languages and then different cultures have some sort of universality.\n",
    "- The network formed by articles and hyperlinks together could provide a <u>self-organized way</u> to gather Wikipedia articles into categories; a classifìcation that it is currently <u>created upon the agreement of the whole Wikipedia community</u>.\n",
    "- The simplest way to create a taxonomy is by use of a tree in the shape of the Linnean taxonomy of living organisms [(Linnaeus, 1735)](https://en.wikipedia.org/wiki/Linnaean_taxonomy)\n",
    "- Such a <u>clean structure does not, unfortunately, fully apply to Wikipedia</u>.\n",
    "- Articles and categories will not strictly form a perfect tree, since an article or a category may happen to be the offspring of more than one parent category.\n",
    "- The taxonomy of articles is represented in this case as a <u>direct acyclic(비순환) graph</u>. \n",
    "- The taxonomy must be <u>considered only as a soft partition</u>, where the intersection between classes is different from zero. \n",
    "- In this case one deals with (so-called) fuzzy partitions.\n",
    "\n",
    "## Bringing order to the WWW\n",
    "- A short overview of the various methods that have been presented and made public <u>to infer the importance (centrality) of pages in the WWW</u>.\n",
    "- Defìne the importance of a page <u>only topologically</u> i.e. <u>without entering into semantic analysis</u> of the content of a single page.\n",
    "\n",
    "### HITS(Hyperlink-Induced Topic Search) algorithm\n",
    "- by [Kleinberg, (Hubs, Authorities, and Communities, 1999)](http://cs.brown.edu/memex/ACM_HypertextTestbed/papers/10.html)\n",
    "- ***Authorities*** i.e. pages that contain <u>relevant information</u> (train timetable, food recipes, formulas of algebra).\n",
    "- ***Hubs*** i.e. pages that do not necessarily contain information,but (as with Yahoo! pages) <u>have links to pages where the information is stored</u>.\n",
    "- Every page $i$ has both an authority score $au(i)$ and a hub score $h(i)$, that are computed via a mutual recursion.\n",
    "- Define <u>the authority of one page as proportional to the sum of the hub scores of the pages pointing to it</u>.\n",
    "\n",
    "$$au(i)\\propto\\sum_{j\\rightarrow i}h(j)$$\n",
    "\n",
    "- The hub score of one page is proportional to the authority scores of the pages <u>reached from the hub</u>,\n",
    "\n",
    "$$h(j)\\propto\\sum_{i\\rightarrow j}au(j)$$\n",
    "\n",
    "- To ensure convergence of the above recursion, a good method is to normalise the values of $h(i)$ and $a(i)$ at every iteration s.t $\\sum^n_{i=1}h(i)=\\sum^n_{i=1}au(i)=1$.\n",
    "- Ref.\n",
    "    - [HITS(Hypertext induced Topic Selection) 알고리즘이란](http://mrseo.co.kr/hitshypertext-induced-topic-selection-알고리즘이란/)\n",
    "    - [HITS algorithm@Wikipedia]( https://en.wikipedia.org/wiki/HITS_algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# HITS algorithm\n",
    "def HITS_algorithm(DG):\n",
    "    auth={}\n",
    "    hub={}\n",
    "    \n",
    "    k = 1000  # number of steps\n",
    "    \n",
    "    for n in DG.nodes():\n",
    "        auth[n] = 1.0\n",
    "        hub[n] = 1.0\n",
    "        \n",
    "    for k in range(k):\n",
    "        norm = 0.0\n",
    "        for n in DG.nodes():\n",
    "            auth[n] = 0.0\n",
    "            for p in DG.predecessors(n):\n",
    "                auth[n] += hub[p]\n",
    "            norm += auth[n]**2.0\n",
    "        norm = norm**0.5\n",
    "        for n in DG.nodes():\n",
    "            auth[n]=auth[n]/norm\n",
    "            \n",
    "        norm = 0.0\n",
    "        for n in DG.nodes():\n",
    "            hub[n] = 0.0\n",
    "            for s in DG.successors(n):\n",
    "                hub[n] += auth[s]\n",
    "            norm += hub[n]**2.0\n",
    "        norm = norm**0.5\n",
    "        for n in DG.nodes():\n",
    "            hub[n] = hub[n] / norm\n",
    "            \n",
    "        return auth, hub\n",
    " \n",
    "\n",
    "DG = nx.DiGraph()\n",
    "DG.add_edges_from([('A', 'B'), ('B', 'C'), ('A', 'D'),\n",
    "                   ('D', 'B'), ('C', 'D'), ('C', 'A')])\n",
    "\n",
    "nx.draw(DG, with_labels=True)\n",
    "\n",
    "(auth, hub) = HITS_algorithm(DG)\n",
    "\n",
    "print(auth)\n",
    "print(hub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral properties\n",
    "- This method can be (qualitatively, not considering the normalisation problems) described by means of linear algebra.\n",
    "- A graph can be equivalently represented by means of a matrix of numbers,that is, with its adjacency matrix.\n",
    "\n",
    "<img src=\"./Fig.4.1.png\" width=450>\n",
    "<center><font size=-1>[A simple oriented graph with its adjacency matrix]</font></center>\n",
    "\n",
    "- The equation giving rise to the hub score, \n",
    "\n",
    "$$h(j)\\propto\\sum_{i\\rightarrow j}au(j)\\rightarrow h(i)\\propto\\sum^n_{j=1}a_{ij}au(j)\\rightarrow\\vec{h}\\propto Aa\\vec{u}$$\n",
    "\n",
    "\n",
    "$$au(i)\\propto\\sum_{j\\rightarrow i}h(j)\\rightarrow au(i)\\propto\\sum^n_{j=1}a^T_{ij}h(j)\\rightarrow\\vec{u}\\propto A^T\\vec{h}$$\n",
    "(where $a^T_{ij}$ are the elements of the matrix $A^T$ that is the transpose of $A$ (this means that $a^T_{ij}=a_{ji}$))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How to transpose and mutiply a matrix\n",
    "def matrix_transpose(M):\n",
    "    M_out=[]\n",
    "    for c in range(len(M[0])):\n",
    "        M_out.append([])\n",
    "        for r in range(len(M)):\n",
    "            M_out[c].append(M[r][c])\n",
    "    \n",
    "    return M_out\n",
    "\n",
    "\n",
    "def matrix_multiplication(M1, M2):\n",
    "    M_out=[]\n",
    "    \n",
    "    for r in range(len(M1)):\n",
    "        M_out.append([])\n",
    "        for j in range(len(M2[0])):\n",
    "            e = 0.0\n",
    "            for i in range(len(M1[r])):\n",
    "                e += M1[r][i] * M2[i][j]\n",
    "            M_out[r].append(e)\n",
    "    return M_out\n",
    "\n",
    "adjacency_matrix1=[[0, 1, 0, 1],\n",
    "                   [1, 0, 1, 1],\n",
    "                   [0, 1, 0, 1]]\n",
    "\n",
    "adjacency_matrix2 = matrix_transpose(adjacency_matrix1)\n",
    "\n",
    "print(\"Transpose adjacency matrix:\", adjacency_matrix2)\n",
    "\n",
    "res_mul = matrix_multiplication(adjacency_matrix1, adjacency_matrix2)\n",
    "print(\"Matrix multiplication:\", res_mul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# How to transpose and mutiply a matrix using Numpy\n",
    "# Modified by etc. using NumPy\n",
    "import numpy as np\n",
    "\n",
    "adjacency_matrix1=[[0, 1, 0, 1],\n",
    "                   [1, 0, 1, 1],\n",
    "                   [0, 1, 0, 1]]\n",
    "\n",
    "adjacency_matrix2 = np.transpose(adjacency_matrix1)\n",
    "\n",
    "print(\"Transpose adjacency matrix:\", adjacency_matrix2)\n",
    "\n",
    "res_mul = np.dot(adjacency_matrix1, adjacency_matrix2)\n",
    "print(\"Matrix multiplication:\", res_mul)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By combining upper formulas,\n",
    "\n",
    "$$\\vec{h}\\propto AA^T\\vec{h}=\\lambda_hAA^T\\vec{h},$$\n",
    "$$a\\vec{u}\\propto A^TAa\\vec{u}=\\lambda_{au}A^TA\\vec{u}.$$\n",
    "\n",
    "That is an eigenvalue problem for the matices $M\\equiv AA^T$ and $M^T\\equiv A^TA$.\n",
    "- $M$ (and therefore its transpose) is real and symmetric, so its eigenvalues are real;\n",
    "- $M$ is non-negative (i.e. the entries are at least 0 or larger); if we can find a $k > 0$ s.t. $M^k >> 0$, that is, all of the entries are strictly larger than 0, then $M$ is *primitive*. If $M$ is a primitive matrix:\n",
    "    * the largest eigenvalue $\\lambda$ of $M$ is positive and of multiplicity 1;\n",
    "    * every other eigenvalue of $M$ is in modulus strictly less than $\\lambda$;\n",
    "    * the largest eigenvalue $\\lambda$ has a corresponding eigenvector with all entries positive.\n",
    "- Being a primitive matrix means in physical terms that the graph defined by the adjacency matrix <u>must have no dangling ends or sinks</u> and that <u>it is possible to reach any page from any starting point</u>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pricipal eigenvalue/vector extraction (power iteration)\n",
    "adjacency_matrix=[[0, 1, 0, 1],\n",
    "                  [1, 0, 1, 1],\n",
    "                  [0, 1, 0, 0],\n",
    "                  [1, 1, 0, 0]]\n",
    "vector=[[0.21], [0.34], [0.52], [0.49]]\n",
    "\n",
    "for i in range(100):\n",
    "    res = matrix_multiplication(adjacency_matrix, vector)\n",
    "    norm_sq = 0.0\n",
    "    for r in res:\n",
    "        norm_sq = norm_sq + r[0] * r[0]\n",
    "    \n",
    "    vector = []\n",
    "    for r in res:\n",
    "        vector.append([r[0]/(norm_sq**0.5)])\n",
    "        \n",
    "print(\"Maxium eigenvalue (in absolute value):\", norm_sq**0.5)\n",
    "print(\"Eigenvector for the maxium eigenvalues:\", vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Modified by etc using NumPy\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "\n",
    "adjacency_matrix=np.array([[0, 1, 0, 1],\n",
    "                           [1, 0, 1, 1],\n",
    "                           [0, 1, 0, 0],\n",
    "                           [1, 1, 0, 0]])\n",
    "vector=[[0.21], [0.34], [0.52], [0.49]]\n",
    "\n",
    "w, v = LA.eig(adjacency_matrix)\n",
    "print(w[0])\n",
    "print(v[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# HITS algorithm for the \".eu\" domain in 2005\n",
    "import operator\n",
    "\n",
    "auth, hub = HITS_algorithm(eu_DG_SCC)\n",
    "sorted_auth = sorted(auth.items(), key=operator.itemgetter(1))\n",
    "sorted_hub = sorted(hub.items(), key=operator.itemgetter(1))\n",
    "\n",
    "print(\"Top 5 auth\")\n",
    "for p in sorted_auth[:5]:\n",
    "    print(dic_nodid_urls[p[0]], p[1])\n",
    "                         \n",
    "print(\"\\nTop 5 hub\")\n",
    "for p in sorted_hub[:5]:\n",
    "    print(dic_nodid_urls[p[0]], p[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PageRank\n",
    "- The most successful measure of eigenvector centrality is given by another algorithm.\n",
    "- Give <u>only one score to the pages of the web</u>, irrespective of its role as authority or hub.\n",
    "- The values of PageRank for the various pages in the graph are given by the eigenvector <mark>$\\vect{r}$</mark> , related to the largest eigenvalue $\\lambda_1$ of the matrix $\\vect{P}$ , given by\n",
    "$$\\vect{P}=\\alpha\\vect{N}+(1-\\alpha)\\vect{E};$$\n",
    "the weight is taken as $\\alpha = 0.85$ in the original paper [(Page et al. , 1999)](http://infolab.stanford.edu/pub/papers/google.pdf). $\\vect{N}$ is the normalised matrix $\\vect{N} = \\vect{AK0}^{-1}$ where $\\vect{A}$ is the adjacency matrix and $\\vect{K0^{-1}}$ is the diagonal matrix,whose entries on the diagonal are given by the inverse of the out degree , $(\\vect{K0}^{-1})_{ii}=1/k^o_i$.\n",
    "- This new matrix $\\vect{P}$ does not differ considerably from the original one $\\vect{N}$ , but has the advantage that (thanks to its irreducibility) its eigenvectors can be computed by a simple iteration procedure [Langville and Meyer (2003)](https://projecteuclid.org/download/pdf_1/euclid.im/1109190965).\n",
    "<img src=\"./Fig.4.2.png\" width=250>\n",
    "<center><font size=-1>[A simple case of recucible matrix]</font></center>\n",
    "- When the matrix is a mathematical theorem (by Perron and Frobenius) ensures that this chain must have a unique and positive stationary vector $\\vect{r}^\\infty$(Perron, 1907; Frobenius, 1912).\n",
    "- Ref\n",
    "    - http://ilpubs.stanford.edu:8090/422/1/1999-66.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute the PageRank\n",
    "def pagerank(graph, damping_factor=0.85, max_iterations=100,\n",
    "             min_delta=0.00000001):\n",
    "    nodes = graph.nodes()\n",
    "    graph_size = len(nodes)\n",
    "    if graph_size == 0:\n",
    "        return {}\n",
    "        \n",
    "    # initialize the page rank dict with 1/N for all nodes\n",
    "    pagerank = dict.fromkeys(nodes, (1.0 - damping_factor)*1.0/graph_size)\n",
    "    min_value = (1.0 - damping_factor)/len(nodes)\n",
    "    \n",
    "    for i in range(max_iterations):\n",
    "        diff = 0  # total difference compared to last iteratction\n",
    "        # computes each node PageRank based on inbound links\n",
    "        for node in nodes:\n",
    "            rank = min_value\n",
    "            for referring_page in graph.predecessors(node):\n",
    "                rank += damping_factor * pagerank[referring_page]/len(graph.neighbors(referring_page))\n",
    "            diff += abs(pagerank[node] - rank)\n",
    "            pagerank[node] = rank\n",
    "            \n",
    "        # stop if PageRank has converged\n",
    "        if diff < min_delta:\n",
    "            break\n",
    "    return pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PageRank for a test network\n",
    "G = nx.DiGraph()\n",
    "G.add_edges_from([(1, 2), (2, 3), (3, 4), (3, 1), (4, 2)])\n",
    "\n",
    "nx.draw(G, with_labels=True)\n",
    "\n",
    "# our PageRank algorithm\n",
    "res_pr = pagerank(G, max_iterations=10000, min_delta=0.00000001, damping_factor=0.85)\n",
    "print(res_pr)\n",
    "\n",
    "# Network PageRank function\n",
    "print(nx.pagerank(G, max_iter=10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img scr=\"Fig.4.3.png\", width=350>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Fig.4.3.png\" width=500>\n",
    "<center><font size=-1>[This is the procedure to generate a network starting from a flux of tweets. The nodes are the twitter users and each time one of them mentions, retweets or replies to another user a link is drawn from the first to the second. The weight of a links is the number of citations between the two.]</font></center> \n",
    "-  In this case a link is drawn from a user “A\" towards a user “B\" if the user “A\" mentions user “B\" in one of their tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Generate and plot the Twitter mention network\n",
    "def generate_network(list_mentions):\n",
    "    DG = nx.DiGraph()\n",
    "    for l in list_mentions:\n",
    "        if len(l) < 2:\n",
    "            continue\n",
    "        for n in l[1:]:\n",
    "            if not DG.has_edge(l[0], n):\n",
    "                DG.add_edge(l[0], n, weight=1.0)\n",
    "            else:\n",
    "                DG[l[0]][n]['weight'] += 1.0\n",
    "    return DG\n",
    "\n",
    "# extracting user and mentions for each tweet\n",
    "# res = twitter_connection.search(q='#FutureDecoded', count=5000)\n",
    "res = twitter_connection.search(q='#GSL', count=5000)\n",
    "# the first will be the tweet user\n",
    "list_users={}\n",
    "list_mentions=[]\n",
    "for t in res['statuses']:\n",
    "    list_unique_ids=[]\n",
    "    print(\"User Screen Name and ID:\", (t['user']['screen_name'], \n",
    "t['user']['id_str']))\n",
    "    list_unique_ids.append(t['user']['id_str'])\n",
    "#     if not list_user.has_key(t['user']['id_str']):\n",
    "    if not t['user']['id_str'] in list_users:\n",
    "        list_users[t['user']['id_str']]=t['user']['screen_name']\n",
    "    print(\"List of Mentions:\", end='')\n",
    "    for m in t['entities']['user_mentions']:\n",
    "        if m['id_str'] != t['user']['id_str']:\n",
    "            list_unique_ids.append(m['id_str'])\n",
    "#             if not list_users.has_key(m['id_str']):\n",
    "            if not m['id_str'] in list_users:\n",
    "                list_users[m['id_str']] = m['screen_name']\n",
    "        print((m['screen_name'], m['id_str']) , end='')\n",
    "    print(\"\\r\")\n",
    "    print(list_unique_ids)\n",
    "    list_mentions.append(list_unique_ids)\n",
    "    print(\"\\n\")\n",
    "\n",
    "net_mentions = generate_network(list_mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pos=nx.nx_pydot.graphviz_layout(net_mentions, prog='neato')\n",
    "nx.draw(net_mentions, pos, node_size=50, node_color='Black')\n",
    "# savefig('./data/hashtag_discussion_thread.png', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Top PageRanks on a Twitter generated network (influencers)\n",
    "import operator\n",
    "\n",
    "pr = nx.pagerank(net_mentions, max_iter=10000)\n",
    "sorted_pr = sorted(pr.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "for page in sorted_pr[:10]:\n",
    "    print(list_users[page[0]], page[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Communities and Girvan-Newman algorithm\n",
    "- The concept of communities is not in itself extremely precise, and also therefore methods for determining them in networks are many and refer to slightly different objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Girvan-newman(GN) algorighm\n",
    "- Based on a recursive deletion of edges.\n",
    "- Edges are selected for their <u>bridging properties</u>, that is to say they are selected if they connect <u>dense regions</u> and therefore after their removal these dense regions appear as the communities within the system.\n",
    "- The quantity chosen for this procedure is the <u>edge betweenness</u>.\n",
    "- Start removing the edge with the largest value then we recompute the edge betweenness and then we delete the one with the largest betweenness among those left.\n",
    "- The process is repeated until all the edges are removed.\n",
    "<img src=\"./Fig.4.5.png\" width=500>\n",
    "<font size=-1>[(left) A toy graph to which we applied the GN algorithm. First we compute the edge betweenness and then we cut the edge with the largest value (dashed). Recursively, we compute and again delete all the edges one after another. Whenever the removal of one edge splits the graph,we indicate (right) the edge in bold (i.e. edges E-F, A-D, G-L, D-I, B-E, A-B, H-I, F-L, C-G). As a result we obtain the dendrogram on the right.]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Code for the GN algorithm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_edges_from([('A', 'B'), ('A', 'D'), ('B', 'D'), ('B', 'E'), ('E', 'I'),\n",
    "                  ('D', 'I'), ('D', 'H'), ('H', 'I'), ('E', 'F'), ('F', 'C'),\n",
    "                  ('F', 'L'), ('C', 'L'), ('C', 'G'), ('G', 'L')])\n",
    "cnt = 1\n",
    "plt.figure(cnt)\n",
    "pos = nx.nx_pydot.graphviz_layout(G, prog='neato')\n",
    "nx.draw(G, pos, with_labels=True)\n",
    "\n",
    "sorted_bc=[1]\n",
    "actual_number_components=1\n",
    "\n",
    "cnt=1\n",
    "while not sorted_bc==[]:\n",
    "    d_edge=nx.edge_betweenness_centrality(G)\n",
    "    sorted_bc = sorted(d_edge.items(), key=operator.itemgetter(1))\n",
    "    e = sorted_bc.pop()\n",
    "    print(\"deleteing edge:\", e[0], end='')\n",
    "    G.remove_edge(*e[0])\n",
    "#     #\n",
    "#     cnt += 1\n",
    "#     plt.figure(cnt)\n",
    "#     nx.draw(G, pos, with_labels=True)\n",
    "#     #\n",
    "    num_comp = nx.number_connected_components(G)\n",
    "    print(\"...we have now \", num_comp, \" components\")\n",
    "    if num_comp > actual_number_components:\n",
    "        actual_number_components = num_comp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modularity\n",
    "- By cutting bridging edges does not tell us when one division is better than another.\n",
    "- A quantity for assessing how good the division is and therefore when we should stop.\n",
    "- Steps:\n",
    "    - the starting point is to consider a partition of the graph into g subgraphs;\n",
    "    - if the partition is good most of the edges will be inside the subgraphs and few will connect them;\n",
    "    - we then define a $g\\times g$ matrix $E$ whose entries $e_{ij}$ give the fraction of edges that in the original graph connect subgraph $i$ to subgraph $j$;\n",
    "    - the actual fraction of edges in subgraph $i$ is given by element $e_{ii}$;\n",
    "    - the quantity $f_i=\\sum_{j=1,g}e_{ij}$ gives the probability that an end-vertex of a randomly extracted edge is in subgraph $i(i\\in 1,...,g)$;\n",
    "    - in the absence of correlations the probability that an edge belongs to subgraph $i$ is $f^2_i$.\n",
    "- Define the modularity $Q$,\n",
    "$$Q=\\sum^g_{i=1} e_{ii}-f^2_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Community detection with the Karate Club network \n",
    "import community\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "G = nx.read_edgelist(\"./data/karate.dat\")\n",
    "\n",
    "# first compute the best partition\n",
    "partition = community.best_partition(G)\n",
    "\n",
    "size = float(len(set(partition.values())))\n",
    "pos = nx.spring_layout(G)\n",
    "count = 0.\n",
    "plt.axis('off')\n",
    "for com in set(partition.values()):\n",
    "    count = count + 1.\n",
    "    list_nodes = [nodes for nodes in partition.keys() if partition[nodes] == com]\n",
    "    nx.draw_networkx_nodes(G, pos, list_nodes, node_size=300,\n",
    "                           node_color=str(count/size))\n",
    "    nx.draw_networkx_labels(G, pos)\n",
    "    \n",
    "nx.draw_networkx_edges(G, pos, alpha=0.5, width=1)\n",
    "# savefig('./data/karate_community.png', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Community detection for the scwiki web graph (by etc)\n",
    "scwiki_pagelinks_net_dir = nx.read_edgelist(\"./data/scwiki_edgelist.dat\",\n",
    "                                            create_using=nx.DiGraph())\n",
    "scwiki_pagelinks_net = nx.read_edgelist(\"./data/scwiki_edgelist.dat\")\n",
    "\n",
    "diz_titles={}\n",
    "with open(\"./data/scwiki_page_titles.dat\", 'r') as f:\n",
    "    for line in f:\n",
    "        print(line.split()[0], line.split()[1])\n",
    "        diz_titles[line.split()[0]]=line.split()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The problem in plotting this network is that it comprises almost 10,000 nodes.\n",
    "- <u>Generate a representative network</u> in which each node is a community (we consider just the first nine with more than 200 nodes), with size proportional to the number of nodes in the corresponding community and edge weight proportional to the number of edges between each pair of communities (we cut the link below the threshold weight 100).\n",
    "- The representative node is chosen <u>according to the Pagerank</u> inside the corresponding community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Generate and optimise the representative network of the community structure\n",
    "\n",
    "#optimization\n",
    "partition = community.best_partition(scwiki_pagelinks_net)\n",
    "\n",
    "# Generate representative ndoes of the community structure\n",
    "community_structure = nx.Graph()\n",
    "diz_communities={}\n",
    "diz_node_labels={}\n",
    "diz_node_sizes={}\n",
    "max_node_size = 0\n",
    "for com in set(partition.values()):\n",
    "    diz_communities[com] = [nodes for nodes in partition.keys() \n",
    "                            if partition[nodes] == com]\n",
    "    if len(diz_communities[com]) >= 200:\n",
    "        if max_node_size < len(diz_communities[com]):\n",
    "            max_node_size = len(diz_communities[com])\n",
    "        print(\"community\", com, len(diz_communities[com]), end='')\n",
    "        sub_scwiki_dir = scwiki_pagelinks_net_dir.subgraph(diz_communities[com])\n",
    "        res_pr = nx.pagerank(sub_scwiki_dir, max_iter=10000)\n",
    "        sorted_pr = sorted(res_pr.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        print(diz_titles[sorted_pr[0][0]], sorted_pr[0][1])\n",
    "        community_structure.add_node(com)\n",
    "        diz_node_labels[com] = diz_titles[sorted_pr[0][0]]\n",
    "        diz_node_sizes[com] = len(diz_communities[com])\n",
    "        \n",
    "# Generate edge weights according to the number of links among commnuities\n",
    "max_edge_weight=0.0\n",
    "for i1 in range(community_structure.number_of_nodes()-1):\n",
    "    for i2 in range(i1+1, community_structure.number_of_nodes()):\n",
    "        wweight=0.0\n",
    "        for n1 in diz_communities[community_structure.nodes()[i1]]:\n",
    "            for n2 in diz_communities[community_structure.nodes()[i2]]:\n",
    "                if scwiki_pagelinks_net.has_edge(n1, n2):\n",
    "                    wweight = wweight + 1.0\n",
    "            \n",
    "        if wweight > 100.0:\n",
    "            if max_edge_weight < wweight:\n",
    "                max_edge_weight = wweight\n",
    "            community_structure.add_edge(community_structure.nodes()[i1],\n",
    "                                         community_structure.nodes()[i2],\n",
    "                                         weight=wweight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plotting the representative network of the community staructure\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pos = nx.nx_pydot.graphviz_layout(community_structure, prog='circo')\n",
    "node_size_factor = 2000.0\n",
    "edge_weight_factor = 10.0\n",
    "\n",
    "plt.axis('off')\n",
    "\n",
    "for n in community_structure.nodes():\n",
    "    nx.draw_networkx_nodes(community_structure, pos, [n], \n",
    "                           node_size=node_size_factor*diz_node_sizes[n]/max_node_size,\n",
    "                           node_color='Black')\n",
    "    nx.draw_networkx_labels(community_structure, pos, font_color='White',\n",
    "                            axis='off')\n",
    "    \n",
    "for e in community_structure.edges():\n",
    "    nx.draw_networkx_edges(community_structure, pos, [e], alpha=0.5,\n",
    "                           width=edge_weight_factor*community_structure[e[0]][e[1]]['weight']/max_edge_weight)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
